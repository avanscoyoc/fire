---
title: "ICS 209 Data"
author: 'Amy Van Scoyoc'
date: "2/20/2019"
output: html_document
---

## About ICS 209 Data

* Source: https://fam.nwcg.gov/fam-web/hist_209/report_list_209
* Data Time Span: 2002-2013
* Date Extracted: 2/19/19
* Region: Northern California, Southern California
* Metadata: Data were copied from website tables into .xlsx document from N/S California from 2002-2013.  Data from 2007 and after had a different column structure, so 'State Unit', 'Incident Type' and 'Measurement' columns were added and left blank for 2002-2006.mData were then saved to .csv file imported here.

## Packages

```{r}
library(tidyverse)
library(stringr)
```


## Data Cleaning and transformation

```{r}
#import data
ics_209 <- read.csv("data/ICS_209_reports.csv", header = TRUE, stringsAsFactors = FALSE)
head(ics_209)

ics_209 <- ics_209 %>% 
      ##filter and clean all data
      filter(Incident.Name != "Incident Name") %>%  #remove header rows from sheet 
      map_dfr(str_trim) %>%     #trim all the white space
      #as.data.frame() %>%   #for old way I mapped trimming white space, ignore
      filter(Incident.Number != "") %>% #remove rows without incident numbers
      filter(!str_detect(Incident.Number, "^HI")) %>% #remove Hawaii data
      filter(Latitude != "") %>% #remove duplicate rows with personnel names
      ##format rows
      transform(Start.Date = as.Date(Start.Date, format = "%m/%d/%Y")) %>% 
      transform(Controlled.Date = as.Date(Controlled.Date, format = "%m/%d/%Y")) %>% 
      transform(Costs = as.numeric(gsub("[\\$,]", "", Costs))) %>%
      transform(Size = as.numeric(gsub("[\\ACRES,]", "", Size))) %>% 
      ##cleaning incident number col
      transform(Incident.Number = gsub(" ", "-", Incident.Number)) %>% #substitute white space for dash
      transform(Incident.Number = gsub("--", "-", Incident.Number)) %>% #remove any double dash instances
      transform(Incident.Number = gsub("CA-","", Incident.Number)) %>% #remove CAs from incident num.
      #transform(Incident.Number = gsub("^([A-Z]{3})([0-9]+)$", "-", Incident.Number)) %>%  #add dash ids (not working)
      mutate(unit = str_extract(Incident.Number, "[A-Z]+")) %>%  #extract unit from incident num.
      mutate(inc.num = str_extract(Incident.Number, "[^-]+$")) %>% #extract unit numbers
      transform(inc.num = formatC(as.numeric(inc.num), width = 8, format = "d", flag = "0")) %>% #pad to 8 digits
      mutate(INC_CLEAN = paste0(unit, "-", inc.num)) %>% #merge cleaned numbers 
      #add year column
      mutate(year = str_extract(Start.Date, "[^-]+"))
View(ics_209)
```


## Load Alex data

```{r}
# Import and clean alex data IDs
GIS <- read.csv("data/all_fire_data.csv") %>% filter(YEAR_ > 2000 & YEAR_ < 2014) %>% 
  unite("inc", UNIT_ID, INC_NUM, sep = "-") %>% 
  mutate(INC_CLEAN = gsub(" ", "", inc))
```


## Join to spatial dataset

```{r}
# Join data
fire_data_joined <- left_join(GIS, ics_209, by = "INC_CLEAN") %>% 
  filter(!is.na(year), GIS_ACRES > 10000) 
```

## View dataset

```{r}
# Create test dataframe to compare join success
fire_data_joined %>% 
  select(INC_CLEAN,YEAR_, year, GIS_ACRES, Size, FIRE_NAME, Incident.Name, Structures.Destroyed, Costs) 
```

** It looks like only 520 rows, only 39 are greater than 10,000 acres, were joined with Alex's dataset. Not sure why this is still happening.**

Here I create data tables with only GIS data and ICS data that didn't match.  Then I merge them into a table to view what is wrong with the incident numbers.  It seems to be a problem with the GIS incident numbers and the jurisdictions.  There are ~200 additional fires greater than 10000 acres in here that could be partially matched and added to the 39 above.

```{r}
join_only_gis <- left_join(GIS, ics_209, by = "INC_CLEAN") %>% 
  filter(is.na(year)) %>% 
  rename(Size = GIS_ACRES, year = YEAR_, Incident.Name = FIRE_NAME, Start.Date = ALARM_DATE)

join_only_ics <- left_join(ics_209, GIS, by = "INC_CLEAN") %>% 
  filter(is.na(YEAR_)) %>% 
  transform(Costs = as.numeric(Costs)) %>%
  transform(Size = as.numeric(Size)) 

library(gtools)
full <- smartbind(join_only_gis, join_only_ics) %>% 
  select(year, Size, Incident.Name, Start.Date, INC_CLEAN, inc, OBJECTID, Structures.Destroyed, Costs) %>% 
  transform(Size = as.numeric(Size)) %>% 
  transform(Costs = as.numeric(Costs)) %>% 
  transform(Structures.Destroyed = as.integer(Structures.Destroyed)) %>% 
  filter(Size > 10000) 

View(full)
```


## Add Millie Data

Millie got 239 fires greater than 10000 acres in her GIS file, SO, we need to import that data table and check what proportion of my data fits hers. 

```{r}
shape <- read.csv("data/fires_millie.csv") %>% 
  unique


```



```{r}
filter <- GIS %>% 
  transform(GIS_ACRES = as.numeric(GIS_ACRES)) %>% 
  filter(GIS_ACRES > 10000)

filter2 <- fire_data_joined %>% 
  filter(!is.na(year) & Size > 10000)

ggplot(filter2, aes(x = INC_CLEAN, y = GIS_ACRES)) +
  geom_bar(stat = "identity", width = 1)
```



## Example Histogram w/ Joined only

```{r}
fig_dat <- fire_data_joined %>% 
  filter(!is.na(year)) #create dataframe with only 520 joined rows & greater than 1000 acres

View(fig_dat)

ggplot(fig_dat, aes(x = INC_CLEAN, y = Size)) +
  geom_bar(stat = "identity", width = 1)


#+
  ylab("acres burned") +
  scale_y_continuous(label = scales::comma) +
  geom_hline(yintercept=100000, linetype="dashed", color = "black") +
  #geom_text(aes(280,109000,label = "megafire", family="Palatino")) +
  annotate("text", label = "megafire", x = 160, y = 120000, size = 3, fontface = "bold", family = "Palatino", colour = "black") +
  scale_fill_manual("", values = c("shrub" = "#34ace0", "forest" = "#33d9b2", "grass" = "#ffb142")) +
  coord_flip() +
  theme_classic() +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```



