---
title: "ICS 209 Data"
author: "Amy Van Scoyoc"
date: "2/20/2019"
output: github_document
---

## About ICS 209 Data

* Source: https://fam.nwcg.gov/fam-web/hist_209/report_list_209
* Data Time Span: 2002-2013
* Date Extracted: 2/19/19
* Region: Northern California, Southern California
* Metadata: Data were copied from website tables into .xlsx document from N/S California from 2002-2013.  Data from 2007 and after had a different column structure, so 'State Unit', 'Incident Type' and 'Measurement' columns were added and left blank for 2002-2006.mData were then saved to .csv file imported here.

## Packages

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```


```{r, message=FALSE}
library(tidyverse)
library(stringr)
library(gtools)  #used for smartbind() function
```


## Data Cleaning and transformation

```{r}
#import data
ics_209 <- read.csv("data/ICS_209_reports.csv", header = TRUE, stringsAsFactors = FALSE)
head(ics_209)

ics_209 <- ics_209 %>% 
      ##filter and clean all data
      filter(Incident.Name != "Incident Name") %>%  #remove header rows from sheet 
      map_dfr(str_trim) %>%     #trim all the white space
      #as.data.frame() %>%   #for old way I mapped trimming white space, ignore
      filter(Incident.Number != "") %>% #remove rows without incident numbers
      filter(!str_detect(Incident.Number, "^HI")) %>% #remove Hawaii data
      filter(Latitude != "") %>% #remove duplicate rows with personnel names
      ##format rows
      transform(Start.Date = as.Date(Start.Date, format = "%m/%d/%Y")) %>% 
      transform(Controlled.Date = as.Date(Controlled.Date, format = "%m/%d/%Y")) %>% 
      transform(Costs = as.numeric(gsub("[\\$,]", "", Costs))) %>%
      transform(Size = as.numeric(gsub("[\\ACRES,]", "", Size))) %>% 
      ##cleaning incident number col
      transform(Incident.Number = gsub(" ", "-", Incident.Number)) %>% #substitute white space for dash
      transform(Incident.Number = gsub("--", "-", Incident.Number)) %>% #remove any double dash instances
      transform(Incident.Number = gsub("CA-","", Incident.Number)) %>% #remove CAs from incident num.
      #transform(Incident.Number = gsub("^([A-Z]{3})([0-9]+)$", "-", Incident.Number)) %>%  #add dash ids (not working)
      mutate(unit = str_extract(Incident.Number, "[A-Z]+")) %>%  #extract unit from incident num.
      mutate(inc.num = str_extract(Incident.Number, "[^-]+$")) %>% #extract unit numbers
      transform(inc.num = formatC(as.numeric(inc.num), width = 8, format = "d", flag = "0")) %>% #pad to 8 digits
      mutate(INC_CLEAN = paste0(unit, "-", inc.num)) %>% #merge cleaned numbers 
      #add year column
      mutate(year = str_extract(Start.Date, "[^-]+"))
```


## Load Alex data

```{r}
# Import and clean alex data IDs
GIS <- read.csv("data/all_fire_data.csv") %>% filter(YEAR_ > 2000 & YEAR_ < 2014) %>% 
  unite("inc", UNIT_ID, INC_NUM, sep = "-") %>% 
  mutate(INC_CLEAN = gsub(" ", "", inc))
```

## Join to spatial dataset

```{r}
# Join datasets and filter to greater than 10,000 acres
fire_data_joined <- left_join(GIS, ics_209, by = "INC_CLEAN") %>% 
  filter(!is.na(year), GIS_ACRES > 10000) 
```

## View dataset

```{r}
# Create test dataframe to compare join success
fire_data_joined %>% 
  select(INC_CLEAN,YEAR_, year, GIS_ACRES, Size, FIRE_NAME, Incident.Name, Structures.Destroyed, Costs) 
```

**It looks like only 520 rows, only 39 are greater than 10,000 acres, were joined with Alex's dataset.**

Take a look at the plot of these values and fire sizes below.

```{r}
ggplot(fire_data_joined, aes(x = INC_CLEAN, y = Size)) +
  geom_bar(stat = "identity", width = 1)
```


## Troubleshooting

To troubleshoot why so many rows didn't match, I create data tables with only GIS data and ICS data that didn't match to examine further. I merge these into a table to view what is wrong with the incident numbers. It seems to be a problem with the GIS incident numbers and the jurisdictions from Alex's dataset.  There are ~100 additional fires greater than 10000 acres in here that could be partially matched and added to the 39 above.

```{r}
join_only_gis <- left_join(GIS, ics_209, by = "INC_CLEAN") %>% 
  filter(is.na(year)) %>% 
  rename(Size = GIS_ACRES, year = YEAR_, Incident.Name = FIRE_NAME, Start.Date = ALARM_DATE)

join_only_ics <- left_join(ics_209, GIS, by = "INC_CLEAN") %>% 
  filter(is.na(YEAR_)) %>% 
  transform(Costs = as.numeric(Costs)) %>%
  transform(Size = as.numeric(Size)) 

full <- smartbind(join_only_gis, join_only_ics) %>% 
  select(year, Size, Incident.Name, Start.Date, INC_CLEAN, inc, OBJECTID, Structures.Destroyed, Costs) %>% 
  transform(Size = as.numeric(Size)) %>% 
  transform(Costs = as.numeric(Costs)) %>% 
  transform(Structures.Destroyed = as.integer(Structures.Destroyed)) %>% 
  filter(Size > 10000) 

head(full)
```

It is clear from this table that the incident numbers in Alex's data (rows with inc/OBJECTID filled in) don't match the ICS-209 incident numbers (rows with Structures/costs filled in).  Next, we'll need to try some partial matching based on Start.Date, Incident.Name, and Size to see how many more rows we can merge to create a more complete dataset of the cost and structural loss that was associate with the fires greater than 10,000 acres between 2001-2013. 


## Partial Matching Attempt

**In Progress**

Here, I attempt to match: 

- `Incident.Name` by first 3 characters
- `Start.Date` within 5 days of eachother
- `Size` within 1000 acres of eachother

```{r}
#TBD
```


## New Merge with Millie Data 

**In Progress**

Millie got 239 fires greater than 10000 acres in her GIS file, SO, we need to import that data table and check what proportion of the ICS-209 data file is contained in it and if we can merge to this data instead. 

```{r}
shape <- read.csv("data/fires_millie.csv") 
head(shape)
```

KC: Starting by removing duplicates from shape df so we know how many fire over 10,000 acres we're working wit. 
```{r Cleaning new shape df}
#need to remove duplicates from new shape data

head(shape)
#counting instances of duplicates by FIRE_NAME
n_occur <- data.frame(table(shape$FIRE_NAME))
n_occur[n_occur$Freq > 1,]
#Looks like 15 fires in the data set have some form of duplicate

duplicates <- shape[shape$FIRE_NAME %in% n_occur$Var1[n_occur$Freq > 1],]
duplicates
# One true duplicate, Ranch fire is listed twice. Thinking about making Fire_Name = "Fire_Name + _YEAR" so each fire has a unique name. Testing with just the duplicates first 

duplicates$Fire_ID <- paste(duplicates$FIRE_NAME, as.character(duplicates$YEAR))
duplicates

#applying this to the shape df
shape$FIRE_ID <- paste(shape$FIRE_NAME, as.character(shape$YEAR))
head(shape)

#Double checking for repeats
n_occur2 <- data.frame(table(shape$FIRE_ID))
n_occur2[n_occur2$Freq > 1,]
shape[shape$FIRE_ID %in% n_occur2$Var1[n_occur2$Freq > 1],]

#how best to remove remaining duplicates?
#Double checking remaining fires and their GIS_ACRES
#CalFire says that the Ranch fire burned ~400,000 acres (whaa?! cray). So the two instances we have in the shape data are true duplicates (both GIS_ACRES are about the same)
#CalFire also has simialr estimates for burned acreage for both of our Thomas_2017 fires. So these two are true duplicates as well. 
#

shape
#shape_sub <- shape[$GIS_ACRES = 410202.46]

```






